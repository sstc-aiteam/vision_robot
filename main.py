import torch
import cv2
import numpy as np
from sam2.build_sam import build_sam2
from sam2.sam2_image_predictor import SAM2ImagePredictor

# é…ç½®è·¯å¾‘
checkpoint = "/home/itrib30156/llm_vision/sam/sam2/checkpoints/sam2.1_hiera_large.pt"
model_cfg  = "/home/itrib30156/llm_vision/sam/sam2.1_hiera_l.yaml"
image_path = "/home/itrib30156/llm_vision/sam/638830629509100000.jpeg"
result_image_path = "simple_result_4.jpg"
result_image_mask_path = "simple_result_4_mask.png"

def get_boundary_points_and_center(mask):
    """ç²å–ç‰©é«”çš„ä¸Šä¸‹å·¦å³å››å€‹é‚Šç•Œé»å’Œé‡å¿ƒ"""
    coords = np.column_stack(np.where(mask > 0))
    
    if len(coords) == 0:
        return None, None
    
    # ç²å–é‚Šç•Œé»
    top_point = coords[np.argmin(coords[:, 0])]      # æœ€ä¸Šæ–¹çš„é»
    bottom_point = coords[np.argmax(coords[:, 0])]   # æœ€ä¸‹æ–¹çš„é»
    left_point = coords[np.argmin(coords[:, 1])]     # æœ€å·¦é‚Šçš„é»
    right_point = coords[np.argmax(coords[:, 1])]    # æœ€å³é‚Šçš„é»
    
    # è½‰æ›ç‚º (x, y) æ ¼å¼
    boundary_points = {
        'top': (top_point[1], top_point[0]),
        'bottom': (bottom_point[1], bottom_point[0]),
        'left': (left_point[1], left_point[0]),
        'right': (right_point[1], right_point[0])
    }
    
    # è¨ˆç®—é‡å¿ƒ (ä½¿ç”¨OpenCV momentsæ–¹æ³•)
    M = cv2.moments((mask * 255).astype(np.uint8))
    if M["m00"] != 0:
        cx = int(M["m10"] / M["m00"])
        cy = int(M["m01"] / M["m00"])
        center = (cx, cy)
    else:
        # å‚™ç”¨æ–¹æ³•ï¼šåƒç´ å¹³å‡
        mean_y = np.mean(coords[:, 0])
        mean_x = np.mean(coords[:, 1])
        center = (int(mean_x), int(mean_y))
    
    return boundary_points, center

def draw_points_on_image(image, boundary_points, center, mask=None):
    """åœ¨åœ–åƒä¸Šç¹ªåˆ¶é‚Šç•Œé»ã€é‡å¿ƒå’Œé®ç½©è¼ªå»“"""
    result_image = image.copy()
    
    # ç¹ªåˆ¶é®ç½©è¼ªå»“
    if mask is not None:
        contours, _ = cv2.findContours(
            (mask * 255).astype(np.uint8), 
            cv2.RETR_EXTERNAL, 
            cv2.CHAIN_APPROX_SIMPLE
        )
        cv2.drawContours(result_image, contours, -1, (0, 255, 0), 2)
    
    # ç¹ªåˆ¶é‚Šç•Œé»
    colors = {
        'top': (255, 0, 0),     # è—è‰²
        'bottom': (0, 255, 0),  # ç¶ è‰²  
        'left': (0, 0, 255),    # ç´…è‰²
        'right': (255, 0, 255)  # ç´«è‰²
    }
    
    labels = {
        'top': 'T',
        'bottom': 'B',
        'left': 'L', 
        'right': 'R'
    }
    
    for point_name, (x, y) in boundary_points.items():
        color = colors[point_name]
        cv2.circle(result_image, (int(x), int(y)), 8, color, -1)
        cv2.circle(result_image, (int(x), int(y)), 10, (255, 255, 255), 2)
        
        cv2.putText(result_image, labels[point_name], 
                   (int(x) - 10, int(y) - 15), cv2.FONT_HERSHEY_SIMPLEX, 
                   0.8, color, 2)
    
    # ç¹ªåˆ¶é‡å¿ƒ
    cx, cy = center
    cv2.circle(result_image, (cx, cy), 12, (255, 255, 0), -1)  # é’è‰²
    cv2.circle(result_image, (cx, cy), 15, (0, 0, 0), 2)
    
    # ç¹ªåˆ¶åå­—æ¨™è¨˜
    cv2.line(result_image, (cx-10, cy), (cx+10, cy), (0, 0, 0), 2)
    cv2.line(result_image, (cx, cy-10), (cx, cy+10), (0, 0, 0), 2)
    
    cv2.putText(result_image, 'CENTER', 
               (cx - 30, cy - 20), cv2.FONT_HERSHEY_SIMPLEX, 
               0.6, (0, 0, 0), 2)
    
    return result_image

# ä¸»ç¨‹åº
if not torch.cuda.is_available():
    print("âš ï¸ æœªæª¢æ¸¬åˆ° CUDAï¼Œå°‡ä½¿ç”¨ CPUï¼ˆé€Ÿåº¦è¼ƒæ…¢ï¼‰")

try:
    # è¼‰å…¥æ¨¡å‹
    print("è¼‰å…¥ SAM2...")
    predictor = SAM2ImagePredictor(build_sam2(model_cfg, checkpoint))
    print("âœ“ æ¨¡å‹è¼‰å…¥æˆåŠŸ")
    
    # è¼‰å…¥åœ–åƒ
    image = cv2.imread(image_path)
    if image is None:
        raise FileNotFoundError(f"ç„¡æ³•è¼‰å…¥åœ–åƒ: {image_path}")
    
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    h, w = image_rgb.shape[:2]
    print(f"âœ“ åœ–åƒè¼‰å…¥æˆåŠŸ: {w}x{h}")
    
    # é æ¸¬
    with torch.inference_mode(), torch.autocast("cuda", dtype=torch.bfloat16):
        predictor.set_image(image_rgb)
        
        input_point = np.array([[w//2, h//2]])
        input_label = np.array([1])
        
        masks, scores, logits = predictor.predict(
            point_coords=input_point,
            point_labels=input_label,
            multimask_output=True # æœƒè‡ªå‹•é¸æ“‡åˆ†æ•¸æœ€é«˜çš„
        )
    
    # é¸æ“‡æœ€ä½³é®ç½©
    best_mask_idx = np.argmax(scores)
    best_mask = masks[best_mask_idx]
    best_score = scores[best_mask_idx]
    
    print(f"âœ“ é æ¸¬å®Œæˆ! æœ€ä½³åˆ†æ•¸: {best_score:.4f}")
    
    # ç²å–é‚Šç•Œé»å’Œé‡å¿ƒ
    boundary_points, center = get_boundary_points_and_center(best_mask)
    
    if boundary_points and center:
        print("\nâœ“ é‚Šç•Œé»åæ¨™:")
        for point_name, coord in boundary_points.items():
            print(f"{point_name.upper()}: {coord}")
        
        print(f"\nâœ“ é‡å¿ƒåæ¨™: {center}")
        
        # ç¹ªåˆ¶çµæœ
        result_image = draw_points_on_image(image, boundary_points, center, best_mask)
        
        # ä¿å­˜çµæœ
        cv2.imwrite(result_image_path, result_image)
        cv2.imwrite(result_image_mask_path, (best_mask * 255).astype(np.uint8))
        
        print("\nâœ“ æ–‡ä»¶å·²ä¿å­˜:")
        print(f"\nğŸ¯ æœ€çµ‚çµæœ:")
        print(f"ä¸Šé»: {boundary_points['top']}")
        print(f"ä¸‹é»: {boundary_points['bottom']}")
        print(f"å·¦é»: {boundary_points['left']}")
        print(f"å³é»: {boundary_points['right']}")
        print(f"é‡å¿ƒ: {center}")
        
    else:
        print("âœ— ç„¡æ³•æ‰¾åˆ°ç‰©é«”çš„é‚Šç•Œé»æˆ–é‡å¿ƒ")

except Exception as e:
    print(f"âœ— éŒ¯èª¤: {e}")
    print("\nå¯èƒ½çš„è§£æ±ºæ–¹æ¡ˆ:")
    print("1. æª¢æŸ¥è·¯å¾‘æ˜¯å¦æ­£ç¢º")
    print("2. ç¢ºèª SAM2 æ˜¯å¦æ­£ç¢ºå®‰è£") 
    print("3. æª¢æŸ¥æª¢æŸ¥é»æ–‡ä»¶æ˜¯å¦å­˜åœ¨")
    print("4. ç¢ºèªåœ–åƒæ–‡ä»¶å­˜åœ¨ä¸”å¯è®€å–")